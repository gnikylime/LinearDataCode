{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Data Lab 11\n",
    "\n",
    "Original lab written by: Emily J. King\n",
    "\n",
    "Goals: Recognize connection between eigenvalues, determinant, rank, and trace. Correctly interpret output of computer-calculated eigenvalues/eigenvectors. Understand power iteration. Practice applications of spectral graph theory.\n",
    "\n",
    "Additional files needed: poweriter.py, Linear_Data_Chapter_2_Lab.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.linalg import det\n",
    "from numpy.linalg import matrix_rank as rank \n",
    "from numpy.linalg import eigvals\n",
    "from numpy.linalg import eig\n",
    "from numpy.linalg import inv\n",
    "from poweriter import poweriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Basics of eigen theory\n",
    "\n",
    "Eigenvalues of diagonal matrices\n",
    "\n",
    "Recall what multiplying by a diagonal matrix does: it scales the first coordinate by the first diagonal element, the second coordinate by the second diagonal element, and so on.  So, what should the eigenvalues and eigenvectors be?  Discuss.\n",
    "\n",
    "Now we test by making a 5x5 diagonal matrix with small integer entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F=np.diag(np.array([1,-1,2,-2,3]))\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should the trace, determinant, and rank be?\n",
    "\n",
    "For any matrix, the trace is just the sum of the diagonal elements.  For a diagonal matrix, the determinant is the product of the diagonal elements.  For a diagonal matrix, the rank is the number of non-zero diagonal elements.  Test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what multiplying by a diagonal matrix does: it scales the first coordinate by the first diagonal element, the second coordinate by the second diagonal element, and so on.  So, what should the eigenvalues and eigenvectors be?  Discuss.\n",
    "\n",
    "Let's compute the eigenvalues alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the eigenvalues are simply the diagonal elements of the diagonal matrix, but they may be listed in a different order due to the algorithm used to compute them.  \n",
    "\n",
    "Now let's see the command to compute the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, U = eig(F)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the eig command returns an array and a matrix. The array is the same thing returned by eigvals. When possible, the matrix returned is an orthogonal matrix: that is, the columns form an orthonormal basis for R^n. Otherwise, the columns are unit norm but not necessarily orthogonal. The columns of the matrix are eigenvectors corresponding to the eigenvalues listed in the array.  IF it is possible to decompose a matrix in the form UDU^(-1) that we've played around with for the last 3 labs, then U above and D formed as the diagonal matrix with diagonal d above, will work.\n",
    "\n",
    "Let's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(U@np.diag(d)@inv(U),F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we multiply F times a column of U?  Let's test the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F@U[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scalar multiply the first column by the first element of d.  They are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]*U[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that F times the 3rd column of U is the scalar multiple of the third element of d times the 3rd column of U."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=2\n",
    "np.allclose(F@U[:,j],d[j]*U[:,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let A be a random 5x5 matrix and define M=AFA^(-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.random.rand(5,5)\n",
    "M=A@F@inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the trace, determinant, and rank of the new matrix M?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all (up to floating point arithmetic) the same as for F. For any matrix B and invertible matrix C, B and CBC^(-1) always have the same trace, determinant, and rank.  And if B is diagonal, then it is easy to compute those by hand.\n",
    "\n",
    "Now let's compute the eigenvalues and some eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_M, U_M = eig(M)\n",
    "d_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues are the same (possibly in a different order and up to floating point arithmetic) as F.  This is because for ny matrix B and invertible matrix C, B and CBC^(-1) always have the eigenvalues (but not in general the same eigenvectors).  What are the eigenvectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the relationship between U_M and A?  The columns of both matrices are eigenvectors for M.  Let's compare the diagonal of F and the rounded eigenvalues returned by the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.real((d_M)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any eigenvalue that appears just once, the column of U_M that corresponds to that entry of d_M is plus or minus 1 times the normalization of the column of A that corresponds to that entry of the diagonal of F.  Test this, fixing indices based on your actual matrices.\n",
    "\n",
    "If an eigenvalue is repeated, then the corresponding columns of U_M are an orthonormal basis for the span of the columns of A matching that eigenvalue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fj=1\n",
    "dj=1\n",
    "np.allclose(A[:,fj]/norm(A[:,fj]),U_M[:,dj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(-A[:,fj]/norm(A[:,fj]),U_M[:,dj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly one of the two commands above should be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors of non-diagonalizable matrices\n",
    "\n",
    "Let's now consider a matrix that cannot be written in in the form CDC^(-1) for C invertible and D diagonal, a shear matrix.\n",
    "\n",
    "Let S be a 2x2 matrix that shears by a factor of 3 downwards.  What do we know must stay in place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=np.array([[1, 0],[-3, 1]])\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to compute the eigenvalues and eigenvectors of S.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, U = eig(S)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 2 ones. Does that mean that there are two dimensions of vectors in R^2 fixed by shearing?  Does that make sense?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can hopefully see what has happened.  Both columns of U are the same (up to floating point arithmetic).  In particular, that means that U is not invertible, and thus UDU^(-1) isn't defined.  The issue is that S is 2x2 but only has one dimension of eigenvectors: the y-axis.  Said another way: S only has one eigenspace, for eigenvalue 1, and that eigenspace is only 1-dimensional.  But numpy will always return d eigenvalues and d eigenvectors for a dxd matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Power iteration\n",
    "\n",
    "When a matrix is \"nice enough\", if you take a random vector and multiply it by higher and higher powers of the matrix, you get output that points closer and closer to the direction of an eigenvector for the largers eigenvalue.  We saw this a bit when playing around with the transition matrix: higher powers applied to the two starting vectors we tried (sunny today vs. gray today) always emphasized the steady state vector.  \n",
    "\n",
    "We can do this for other matrices which have largest eigenvalue larger than 1. To make sure that the vector norms aren't getting too big for the computer, we can normalize.  So, the idea is, multiply a random vector by our matrix, normalize this output, then mulitply that by the matrix and repeat.  Take a look at the code in poweriter.py and discuss.\n",
    "\n",
    "Let's try this with the transition matrix from lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P=np.array([[9/10, 1/2],[1/10, 1/2]])\n",
    "xout=poweriter(P,np.random.rand(2),1000)\n",
    "xout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the normalization of the vector we know to be the steady state vector (i.e., a vector with eigenvalue 1, while the other eigenvalue is 2/5 < 1) to the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(xout,np.array([5/6, 1/6])/norm(np.array([5/6, 1/6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(xout,-np.array([5/6, 1/6])/norm(np.array([5/6, 1/6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that power iteration output the normalization of the steady state vector (or negative the normalization).\n",
    "\n",
    "Now let's do the same thing with the matrix M above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout=poweriter(M,np.random.rand(5),1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns (an approximation of) an eigenvector of M with the largest eigenvalue (in absolute value). Let's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M@xout/xout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Playing with spectral graph theory\n",
    "\n",
    "Computing the graph Laplacian of the (unweighted, undirected) graph from Linear_Data_Chapter_2_Lab.pdf and the book.  First the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0, 1, 0, 0, 1, 0], [1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 0], [0, 0, 1, 0, 1, 1], [1, 1, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0]])\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the degree matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=np.diag(A@np.ones(A.shape[0]))\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the (unnormalized) graph Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=D-A\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_L, U_L = eig(L)\n",
    "d_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is (up to floating point arithmetic) exactly one zero as an eigenvalue since the graph has exactly one connected component.  Also, since the graph Laplacian is a special kind of matrix called positive semidefininte, the eigenvalues are all non-negative real numbers. \n",
    "\n",
    "Let's look at the entries of the Fielder eigenvector (i.e. the returned eigenvector corresponding to the smallest positive eigenvalue).  If the second element of d_L isn't the smallest positive eigenvalue, adjust the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_L[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests clustering vertices 1, 2, 3, and 5 together and 4 and 6 together.\n",
    "\n",
    "Now, let's looks at the ratio between the largest eigenvalue and the Fiedler eigenvalue (i.e., the smallest positive eigenvalue).  We'll track changes to this number as we manipulate the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_L[1]/d_L[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an edge that doesn't yet exist in {1,2,3,5}, 1<->3. And then compute the eigenvalues and eigenvectors as well as the ratio between the largest eigenvalue and the Fielder eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,2]=1\n",
    "A[2,0]=1\n",
    "L=np.diag(A@np.ones(A.shape[0]))-A\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_L, U_L = eig(L)\n",
    "d_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the entries of the Fielder eigenvector, double checking where the smallest non-zero eigenvalue is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_L[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still suggests clustering vertices 1, 2, 3, and 5 together and 4 and 6 together.\n",
    "\n",
    "Now, let's looks at the ratio between the largest eigenvalue and the Fiedler eigenvalue.  For me, the largest is at the 3rd position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_L[1]/d_L[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this ratio is slightly smaller, suggesting stronger connectivity within our predicted clusters.\n",
    "\n",
    "Let's add an edge 1<->6, which is \"across\" the previously predicted clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,5]=1\n",
    "A[5,0]=1\n",
    "L=np.diag(A@np.ones(A.shape[0]))-A\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_L, U_L = eig(L)\n",
    "d_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the entries of the Fielder eigenvector. For me, the smallest positive eigenvalue is in the fifth position.  Change as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_L[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still suggests clustering vertices 1, 2, 3, and 5 together and 4 and 6 together.\n",
    "\n",
    "But let's looks at the ratio between the largest eigenvalue and the Fiedler eigenvalue.  For me, the largest is at the second position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_L[4]/d_L[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio is larger, suggesting that the clustering is less certain.\n",
    "\n",
    "Play around with other graphs to see how connectivity affects the spectrum of the graph Laplacian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "\n",
    "1a. Make a random symmetric 3x3 matrix using the code below.  (A symmetrix matrix is equal to its own transpose.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=np.random.rand(3,3)\n",
    "C=C+np.transpose(C)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Run power iteration to find the largest eigenvalue and a corresponding eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use numpy commands to directly find the eigenvectors and eigenvalues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a. Enter the adjacency matrix of the weighted graph from Linear_Data_Chapter_2_Lab.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Compute the (unnormalized) graph Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Cluster the graph using the Fielder eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Interpret the Fielder eigenvector here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
